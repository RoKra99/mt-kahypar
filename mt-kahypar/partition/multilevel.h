/*******************************************************************************
 * This file is part of KaHyPar.
 *
 * Copyright (C) 2017 Sebastian Schlag <sebastian.schlag@kit.edu>
 *
 * KaHyPar is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * KaHyPar is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with KaHyPar.  If not, see <http://www.gnu.org/licenses/>.
 *
 ******************************************************************************/

#pragma once

#include <memory>

#include "tbb/task.h"

#include "mt-kahypar/definitions.h"
#include "mt-kahypar/partition/context.h"
#include "mt-kahypar/partition/factories.h"
#include "mt-kahypar/partition/initial_partitioning/flat/pool_initial_partitioner.h"
#include "mt-kahypar/parallel/memory_pool.h"
#include "mt-kahypar/utils/initial_partitioning_stats.h"
#include "mt-kahypar/utils/profiler.h"
#include "mt-kahypar/utils/timer.h"
#include "mt-kahypar/utils/stats.h"

namespace mt_kahypar {
namespace multilevel {

namespace {

using PartitionedHypergraph = mt_kahypar::PartitionedHypergraph;

class RefinementTask : public tbb::task {

 public:
  RefinementTask(Hypergraph& hypergraph,
                 PartitionedHypergraph& partitioned_hypergraph,
                 const Context& context,
                 const bool top_level,
                 const TaskGroupID task_group_id,
                 const bool vcycle,
                 const bool wcycle) :
    _coarsener(nullptr),
    _sparsifier(nullptr),
    _ip_context(context),
    _hg(hypergraph),
    _partitioned_hg(partitioned_hypergraph),
    _context(context),
    _top_level(top_level),
    _task_group_id(task_group_id),
    _vcycle(vcycle),
    _wcycle(wcycle) {
    // Must be empty, because final partitioned hypergraph
    // is moved into this object
    _coarsener = CoarsenerFactory::getInstance().createObject(
      _context.coarsening.algorithm, _hg, _context, _task_group_id, _top_level);
    _sparsifier = HypergraphSparsifierFactory::getInstance().createObject(
      _context.sparsification.similiar_net_combiner_strategy, _context, _task_group_id);

    // Switch refinement context from IP to main
    _ip_context.refinement = _context.initial_partitioning.refinement;
  }

  tbb::task* execute() override {
    enableTimerAndStats();

    if ( _sparsifier->isSparsified() ) {
      // In that case, the sparsified hypergraph generated by the
      // heavy hyperedge remover was used for initial partitioning.
      // => Partition has to mapped from sparsified hypergraph to
      // coarsest partitioned hypergraph.
      io::printPartitioningResults(_sparsifier->sparsifiedPartitionedHypergraph(),
        _context, "Sparsified Initial Partitioning Results:");
      _sparsifier->undoSparsification(_coarsener->coarsestPartitionedHypergraph());
    }

    utils::Timer::instance().stop_timer("initial_partitioning");

    if ( _top_level && !_wcycle ) {
      utils::Profiler::instance().deactivate("Initial Partitioning");
    }

    PartitionedHypergraph& coarsest_partitioned_hypergraph =
      _coarsener->coarsestPartitionedHypergraph();
    io::printPartitioningResults(coarsest_partitioned_hypergraph,
      _context, "Initial Partitioning Results:");
    if ( _context.partition.verbose_output && (!_vcycle && !_wcycle) ) {
      utils::InitialPartitioningStats::instance().printInitialPartitioningStats();
    }

    // ################## LOCAL SEARCH ##################
    io::printLocalSearchBanner(_context);

    if ( _top_level && !_wcycle ) {
      utils::Profiler::instance().activate("Refinement");
    }

    utils::Timer::instance().start_timer("refinement", "Refinement");
    std::unique_ptr<IRefiner> label_propagation =
      LabelPropagationFactory::getInstance().createObject(
        _context.refinement.label_propagation.algorithm,
        _hg, _context, _task_group_id);
    std::unique_ptr<IRefiner> fm =
      FMFactory::getInstance().createObject(
        _context.refinement.fm.algorithm,
        _hg, _context, _task_group_id);

    _partitioned_hg = _coarsener->uncoarsen(label_propagation, fm);
    utils::Timer::instance().stop_timer("refinement");

    if ( _top_level && !_wcycle ) {
      utils::Profiler::instance().deactivate("Refinement");
    }

    io::printPartitioningResults(_partitioned_hg, _context, "Local Search Results:");
    return nullptr;
  }

 public:
  std::unique_ptr<ICoarsener> _coarsener;
  std::unique_ptr<IHypergraphSparsifier> _sparsifier;
  Context _ip_context;

 private:
  void enableTimerAndStats() {
    if ( _top_level && !_wcycle ) {
      parallel::MemoryPool::instance().activate_unused_memory_allocations();
      utils::Timer::instance().enable();
      utils::Stats::instance().enable();
    }
  }

  Hypergraph& _hg;
  PartitionedHypergraph& _partitioned_hg;
  const Context& _context;
  const bool _top_level;
  const TaskGroupID _task_group_id;
  const bool _vcycle;
  const bool _wcycle;
};

class CoarseningTask : public tbb::task {

 public:
  CoarseningTask(Hypergraph& hypergraph,
                 IHypergraphSparsifier& sparsifier,
                 const Context& context,
                 const Context& ip_context,
                 ICoarsener& coarsener,
                 const bool top_level,
                 const TaskGroupID task_group_id,
                 const bool vcycle,
                 const bool wcycle) :
    _hg(hypergraph),
    _sparsifier(sparsifier),
    _context(context),
    _ip_context(ip_context),
    _coarsener(coarsener),
    _top_level(top_level),
    _task_group_id(task_group_id),
    _vcycle(vcycle),
    _wcycle(wcycle) { }

  tbb::task* execute() override {
    // ################## COARSENING ##################
    mt_kahypar::io::printCoarseningBanner(_context);

    if ( _top_level && !_wcycle ) {
      utils::Profiler::instance().activate("Coarsening");
    }

    utils::Timer::instance().start_timer("coarsening", "Coarsening");
    _coarsener.coarsen();
    utils::Timer::instance().stop_timer("coarsening");

    if ( _top_level && !_wcycle ) {
      utils::Profiler::instance().deactivate("Coarsening");
    }

    if (_context.partition.verbose_output) {
      mt_kahypar::io::printHypergraphInfo(
        _coarsener.coarsestHypergraph(), "Coarsened Hypergraph",
        _context.partition.show_memory_consumption);
    }

    // ################## INITIAL PARTITIONING ##################
    utils::Timer::instance().start_timer("initial_partitioning", "Initial Partitioning");
    if ( _context.useSparsification() ) {
      // Sparsify Hypergraph, if heavy hyperedge removal is enabled
      utils::Timer::instance().start_timer("sparsify_hypergraph", "Sparsify Hypergraph");
      _sparsifier.sparsify(_coarsener.coarsestHypergraph());
      utils::Timer::instance().stop_timer("sparsify_hypergraph");
    }

    if ( _sparsifier.isSparsified() ) {
      if (_context.partition.verbose_output) {
        mt_kahypar::io::printHypergraphInfo(
          _sparsifier.sparsifiedHypergraph(), "Sparsified Hypergraph",
          _context.partition.show_memory_consumption);
      }
      initialPartition(_sparsifier.sparsifiedPartitionedHypergraph());
    } else {
      initialPartition(_coarsener.coarsestPartitionedHypergraph());
    }

    return nullptr;
  }

 private:
  void initialPartition(PartitionedHypergraph& phg) {
    io::printInitialPartitioningBanner(_context);

    if ( _top_level && !_wcycle ) {
      utils::Profiler::instance().activate("Initial Partitioning");
    }

    if ( !_vcycle && !_wcycle ) {
      if ( _context.initial_partitioning.mode == InitialPartitioningMode::direct ) {
        disableTimerAndStats();
        PoolInitialPartitionerContinuation& ip_continuation = *new(allocate_continuation())
          PoolInitialPartitionerContinuation(
            phg, _ip_context, _task_group_id);
        spawn_initial_partitioner(ip_continuation);
      } else {
        std::unique_ptr<IInitialPartitioner> initial_partitioner =
          InitialPartitionerFactory::getInstance().createObject(
            _ip_context.initial_partitioning.mode, phg,
            _ip_context, _top_level, _task_group_id);
        initial_partitioner->initialPartition();
      }
    } else {
      // V-Cycle: Partition IDs are given by its community IDs
      const Hypergraph& hypergraph = phg.hypergraph();
      phg.doParallelForAllNodes([&](const HypernodeID hn) {
        const PartitionID part_id = hypergraph.communityID(hn);
        ASSERT(part_id != kInvalidPartition && part_id < _context.partition.k);
        phg.setOnlyNodePart(hn, part_id);
      });
      phg.initializePartition(_task_group_id);
    }
  }

  void disableTimerAndStats() {
    if ( _top_level && !_wcycle ) {
      parallel::MemoryPool::instance().deactivate_unused_memory_allocations();
      utils::Timer::instance().disable();
      utils::Stats::instance().disable();
    }
  }

  Hypergraph& _hg;
  IHypergraphSparsifier& _sparsifier;
  const Context& _context;
  const Context& _ip_context;
  ICoarsener& _coarsener;
  const bool _top_level;
  const TaskGroupID _task_group_id;
  const bool _vcycle;
  const bool _wcycle;
};

// ! Helper function that spawns the multilevel partitioner in
// ! TBB continuation style with a given parent task.
static void spawn_multilevel_partitioner(Hypergraph& hypergraph,
                                         PartitionedHypergraph& partitioned_hypergraph,
                                         const Context& context,
                                         const bool top_level,
                                         const TaskGroupID task_group_id,
                                         const bool vcycle,
                                         const bool wcycle,
                                         tbb::task& parent) {
  // The coarsening task is first executed and once it finishes the
  // refinement task continues (without blocking)
  RefinementTask& refinement_task = *new(parent.allocate_continuation())
    RefinementTask(hypergraph, partitioned_hypergraph, context, top_level,
      task_group_id, vcycle, wcycle);
  refinement_task.set_ref_count(1);
  CoarseningTask& coarsening_task = *new(refinement_task.allocate_child()) CoarseningTask(
    hypergraph, *refinement_task._sparsifier,
     context, refinement_task._ip_context,
     *refinement_task._coarsener, top_level, task_group_id,
     vcycle, wcycle);
  tbb::task::spawn(coarsening_task);
}

class MultilevelPartitioningTask : public tbb::task {

 public:
  MultilevelPartitioningTask(Hypergraph& hypergraph,
                             PartitionedHypergraph& partitioned_hypergraph,
                             const Context& context,
                             const bool top_level,
                             const TaskGroupID task_group_id,
                             const bool vcycle,
                             const bool wcycle) :
    _hg(hypergraph),
    _partitioned_hg(partitioned_hypergraph),
    _context(context),
    _top_level(top_level),
    _task_group_id(task_group_id),
    _vcycle(vcycle),
    _wcycle(wcycle) { }

  tbb::task* execute() override {
    spawn_multilevel_partitioner(
    _hg, _partitioned_hg, _context, _top_level,
    _task_group_id, _vcycle, _wcycle, *this);
    return nullptr;
  }

 private:
  Hypergraph& _hg;
  PartitionedHypergraph& _partitioned_hg;
  const Context& _context;
  const bool _top_level;
  const TaskGroupID _task_group_id;
  const bool _vcycle;
  const bool _wcycle;
};

} // namespace

// ! Performs multilevel partitioning on the given hypergraph
// ! in TBB blocking-style.
static inline PartitionedHypergraph partition(Hypergraph& hypergraph,
                                              const Context& context,
                                              const bool top_level,
                                              const TaskGroupID task_group_id,
                                              const bool vcycle = false,
                                              const bool wcycle = false) {
  PartitionedHypergraph partitioned_hypergraph;
  MultilevelPartitioningTask& multilevel_task = *new(tbb::task::allocate_root())
    MultilevelPartitioningTask(
      hypergraph, partitioned_hypergraph,
      context, top_level, task_group_id, vcycle, wcycle);
  tbb::task::spawn_root_and_wait(multilevel_task);
  return partitioned_hypergraph;
}

// ! Performs multilevel partitioning on the given hypergraph
// ! in TBB continuation-style.
// ! Note, the final partitioned hypergraph is moved into the
// ! passed partitioned hypergraph object.
static inline void partition_async(Hypergraph& hypergraph,
                                   PartitionedHypergraph& partitioned_hypergraph,
                                   const Context& context,
                                   const bool top_level,
                                   const TaskGroupID task_group_id,
                                   tbb::task* parent) {
  ASSERT(parent);
  spawn_multilevel_partitioner(
    hypergraph, partitioned_hypergraph, context,
    top_level, task_group_id, false, false, *parent);
}

namespace {

static inline void partitionMultilevelWCycle(Hypergraph& hypergraph,
                                             PartitionedHypergraph& partitioned_hypergraph,
                                             const Context& context) {
  parallel::scalable_vector<PartitionID> part_ids(
    hypergraph.initialNumNodes(), kInvalidPartition);

  // Store partition and assign it as community ids in order to
  // restrict contractions in w-cycle to partition ids
  hypergraph.doParallelForAllNodes([&](const HypernodeID& hn) {
    part_ids[hn] = partitioned_hypergraph.partID(hn);
  });
  hypergraph.setCommunityIDs(part_ids);

  // W-Cycle Multilevel Partitioning
  PartitionedHypergraph w_cycle_phg = multilevel::partition(
    hypergraph, context, true, TBBNumaArena::GLOBAL_TASK_GROUP, false, true /* wcycle */);

  // Apply Partition of W-Cycle
  partitioned_hypergraph.doParallelForAllNodes([&](const HypernodeID hn) {
    ASSERT(w_cycle_phg.nodeIsEnabled(hn));
    const PartitionID from = partitioned_hypergraph.partID(hn);
    const PartitionID to = w_cycle_phg.partID(hn);
    if ( from != to ) {
      ASSERT(to != kInvalidPartition);
      if ( partitioned_hypergraph.isGainCacheInitialized() ) {
        partitioned_hypergraph.changeNodePartFullUpdate(hn, from, to);
      } else {
        partitioned_hypergraph.changeNodePart(hn, from, to);
      }
    }
  });

  ASSERT(metrics::objective(partitioned_hypergraph, context.partition.objective) ==
          metrics::objective(w_cycle_phg, context.partition.objective),
          V(metrics::objective(partitioned_hypergraph, context.partition.objective)) <<
          V(metrics::objective(w_cycle_phg, context.partition.objective)));
  ASSERT(metrics::imbalance(partitioned_hypergraph, context) ==
          metrics::imbalance(w_cycle_phg, context),
          V(metrics::imbalance(partitioned_hypergraph, context)) <<
          V(metrics::imbalance(w_cycle_phg, context)));
}

static inline void partitionNLevelWCycle(const Hypergraph& hypergraph,
                                         PartitionedHypergraph& partitioned_hypergraph,
                                         const Context& context) {
  // Compactify hypergraph since original hypergraph contains information
  // how to uncontract the hypergraph further after the w-cycle
  auto compactification = HypergraphFactory::compactify(
    TBBNumaArena::GLOBAL_TASK_GROUP, hypergraph);
  Hypergraph& compactified_hg = compactification.first;
  auto& hn_mapping = compactification.second;

  parallel::scalable_vector<PartitionID> part_ids(
    compactified_hg.initialNumNodes(), kInvalidPartition);

  // Store partition and assign it as community ids in order to
  // restrict contractions in w-cycle to partition ids
  hypergraph.doParallelForAllNodes([&](const HypernodeID& hn) {
    const HypernodeID compactified_hn = hn_mapping[hn];
    ASSERT(compactified_hn < ID(part_ids.size()));
    part_ids[compactified_hn] = partitioned_hypergraph.partID(hn);
  });
  compactified_hg.setCommunityIDs(part_ids);

  // W-Cycle Multilevel Partitioning
  PartitionedHypergraph w_cycle_phg = multilevel::partition(
    compactified_hg, context, true, TBBNumaArena::GLOBAL_TASK_GROUP, false, true /* wcycle */);

  // Apply Partition of W-Cycle
  partitioned_hypergraph.doParallelForAllNodes([&](const HypernodeID hn) {
    const HypernodeID compactified_hn = hn_mapping[hn];
    ASSERT(w_cycle_phg.nodeIsEnabled(compactified_hn));
    const PartitionID from = partitioned_hypergraph.partID(hn);
    const PartitionID to = w_cycle_phg.partID(compactified_hn);
    if ( from != to ) {
      ASSERT(to != kInvalidPartition);
      if ( partitioned_hypergraph.isGainCacheInitialized() ) {
        partitioned_hypergraph.changeNodePartFullUpdate(hn, from, to);
      } else {
        partitioned_hypergraph.changeNodePart(hn, from, to);
      }
    }
  });

  ASSERT(metrics::objective(partitioned_hypergraph, context.partition.objective) ==
          metrics::objective(w_cycle_phg, context.partition.objective),
          V(metrics::objective(partitioned_hypergraph, context.partition.objective)) <<
          V(metrics::objective(w_cycle_phg, context.partition.objective)));
  ASSERT(metrics::imbalance(partitioned_hypergraph, context) ==
          metrics::imbalance(w_cycle_phg, context),
          V(metrics::imbalance(partitioned_hypergraph, context)) <<
          V(metrics::imbalance(w_cycle_phg, context)));
}

} // namsepace

static inline void partitionWCycle(Hypergraph& hypergraph,
                                   PartitionedHypergraph& partitioned_hypergraph,
                                   Context& context) {
  // Deactivate Memory Pool if active
  const bool memory_pool_active = parallel::MemoryPool::instance().is_active();
  if ( memory_pool_active ) {
    parallel::MemoryPool::instance().deactivate();
  }
  // Deactivate Timer if active
  const bool timer_active = utils::Timer::instance().isEnabled();
  if ( timer_active ) {
    utils::Timer::instance().disable();
  }
  // Deactivate Stats if active
  const bool stats_active = utils::Stats::instance().isEnabled();
  if ( stats_active ) {
    utils::Stats::instance().disable();
  }

  ASSERT(!context.partition.w_cycle_thresholds.empty());
  const HypernodeID w_cycle_threshold = context.partition.w_cycle_thresholds.back();
  context.partition.w_cycle_thresholds.pop_back();
  io::printWCycleBanner(context);

  #ifdef KAHYPAR_USE_N_LEVEL_PARADIGM
  partitionNLevelWCycle(hypergraph, partitioned_hypergraph, context);
  #else
  partitionMultilevelWCycle(hypergraph, partitioned_hypergraph, context);
  #endif

  while ( !context.partition.w_cycle_thresholds.empty() &&
          w_cycle_threshold >= context.partition.w_cycle_thresholds.back() ) {
    context.partition.w_cycle_thresholds.pop_back();
  }

  // Activate Memory Pool again
  if ( memory_pool_active ) {
    parallel::MemoryPool::instance().activate();
  }
  // Activate Timer again
  if ( timer_active ) {
    utils::Timer::instance().enable();
  }
  // Activate Stats again
  if ( stats_active ) {
    utils::Stats::instance().enable();
  }
}

}  // namespace multilevel
}  // namespace mt_kahypar
